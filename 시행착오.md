# SAGA 시행착오 기록

## 1. Anthropic Prompt Caching 무효화 버그 (2026-02-25)

### 증상
- `cache_read`가 매 턴 5553 토큰에서 고정
- `cache_create`가 매 턴 14000~15000 토큰씩 새로 생성
- Turn 2 이후에도 이전 턴의 캐시가 전혀 재사용되지 않음

```
Turn 1: input=24  cache_read=5553  cache_create=14187
Turn 2: input=9   cache_read=5553  cache_create=14892  ← 14187이 재사용 안됨
Turn 3: input=25  cache_read=5553  cache_create=15573  ← 계속 새로 생성
```

### 원인

Anthropic API의 prompt caching은 **prefix 기반**이다. 즉 요청의 처음부터 cache breakpoint(BP)까지의 내용이 동일해야 캐시가 hit된다.

`server.py`의 `_build_cacheable_messages()`에서 동적 컨텍스트(에피소드, 로어, 상태 등)를 `role: "system"` 메시지로 대화 중간에 삽입했다:

```python
# 문제의 코드
messages.insert(last_user_idx, {
    "role": "system",        # ← 이게 문제
    "content": context_block, # 매 턴 내용이 바뀜
})
```

그런데 `llm/client.py`의 `_call_anthropic()`에서는 **모든 system 메시지를 `body["system"]` 배열로 호이스팅**한다:

```python
for msg in messages:
    if msg["role"] == "system":
        system_parts.append(part)  # 전부 system 배열로 올림
    else:
        non_system.append(entry)
```

결과적으로 Anthropic API에 전송되는 구조:

```
system: [
  시스템프롬프트 (BP1, 고정, 5553 토큰),     ← cache hit OK
  동적컨텍스트 (매 턴 변경, ~1500 토큰)       ← 이것 때문에 BP2 prefix 변경!
]
messages: [
  user1,
  assistant1 (BP2),   ← prefix에 동적컨텍스트 포함 → 캐시 무효화
  user2
]
```

Anthropic caching에서 BP2의 캐시 키 = `system[0] + system[1] + user1 + assistant1`인데, `system[1]`(동적컨텍스트)이 매 턴 바뀌므로 BP2 캐시가 **절대로 hit되지 않는다**.

5553 토큰의 `cache_read`는 BP1(시스템프롬프트)만 히트된 것이고, 나머지 14000+ 토큰은 매번 새로 캐시를 생성하고 있었다.

### 해결

동적 컨텍스트를 system 메시지가 아닌 **마지막 user 메시지에 prepend**하도록 변경:

```python
# 수정 후
if last_user_idx is not None:
    messages[last_user_idx] = dict(messages[last_user_idx])
    messages[last_user_idx]["content"] = context_block + "\n\n" + messages[last_user_idx]["content"]
```

수정 후 API 구조:

```
system: [
  시스템프롬프트 (BP1, 고정)         ← cache hit
]
messages: [
  user1,                            ← 고정
  assistant1 (BP2),                 ← prefix 고정 → cache hit!
  "동적컨텍스트\n\nuser2"           ← BP 뒤에 위치, 캐시에 영향 없음
]
```

### 수정 후 결과

```
Turn 1: input=317   cache_read=5,553   cache_create=15,719  (초기 캐시 생성)
Turn 2: input=589   cache_read=21,272  cache_create=441     ← 95% 캐시 히트!
Turn 3: input=809   cache_read=21,713  cache_create=522     ← 96% 캐시 히트!
Turn 4: input=1,006  cache_read=22,235  cache_create=456     ← 96% 캐시 히트!
```

Turn 2부터 이전 턴의 `cache_create`가 다음 턴의 `cache_read`로 전환되어 **~96% 캐시 히트율** 달성.

### 교훈

1. **Anthropic API는 모든 system 메시지를 messages 앞의 system 배열로 호이스팅한다** — 대화 중간에 삽입한 system 메시지도 앞으로 올라간다
2. **Prompt caching은 prefix 기반** — cache breakpoint 이전의 내용이 1 토큰이라도 바뀌면 해당 BP 이후 전체 캐시가 무효화된다
3. **동적 컨텍스트는 반드시 모든 cache breakpoint 뒤에 배치**해야 한다 — user 메시지에 prepend하는 것이 가장 안전한 방법
4. `cache_read`가 고정되어 있고 `cache_create`가 매 턴 비슷한 크기라면 캐시가 무효화되고 있다는 신호

### 수정 파일
- `saga/server.py` → `_build_cacheable_messages()` (lines 286~312)

---

## 2. Prompt Caching 최소 토큰 임계값 미달 (2026-02-28)

### 증상
- 3-BP explicit 캐싱 적용했는데 `cache_creation_input_tokens`가 항상 0
- `cache_read_input_tokens`도 0 — 캐시가 아예 생성되지 않음
- API 에러는 없고, 정상 응답은 옴

### 원인

Anthropic의 **모델별 최소 캐싱 토큰 임계값**을 간과함:

| 모델 | 최소 캐시 가능 토큰 |
|------|:-----------------:|
| Claude Opus 4.6 / 4.5 | 4096 |
| Claude Sonnet 4.6 | 2048 |
| Claude Sonnet 4.5 / 4 / 3.7 | 1024 |
| **Claude Haiku 4.5** | **4096** |
| Claude Haiku 3.5 / 3 | 2048 |

벤치마크 초기 시스템 프롬프트가 ~300 토큰 → 2600 토큰으로 늘렸는데도 Haiku 4.5의 4096 임계값 미달. **임계값 미만이면 `cache_control`을 달아도 캐시 생성/읽기가 완전히 무시됨** (에러 없이 조용히 스킵).

### 해결

시스템 프롬프트를 실제 SAGA RP 규모(~4100+ 토큰)로 확장하여 임계값 초과:

```
수정 전: ~300 토큰 (간단한 나레이터 지시)  → cache_creation: 0
수정 후: ~4100 토큰 (세계관+캐릭터+룰)     → cache_creation: 4686 ✓
```

### 수정 후 결과 (50턴 벤치마크)

```
Turn  1: cache_read=4686  cache_create=0     hit_rate=96.3% (이전 세션 캐시 재활용)
Turn  2: cache_read=4686  cache_create=328   hit_rate=90.2%
Turn 10: cache_read=7323  cache_create=328   hit_rate=93.4%
Turn 25: cache_read=12269 cache_create=329   hit_rate=95.7%
Turn 50: cache_read=20500 cache_create=328   hit_rate=97.3%

총 비용: $0.17 (캐시 미적용 시 $0.72) → 76.7% 절감
```

### 교훈

1. **Haiku 4.5의 최소 캐시 토큰은 4096** — Sonnet보다 높다. 모델별로 임계값이 다르므로 반드시 확인
2. **임계값 미달 시 에러가 아니라 조용히 스킵** — `cache_creation: 0`이면 프롬프트 길이를 의심할 것
3. **실제 RP 시스템 프롬프트(~14K 토큰)에서는 문제없음** — 벤치마크용 짧은 프롬프트에서만 발생하는 이슈

---

## 3. 자동 캐싱(top-level) vs 수동 3-BP: 장기 대화에서의 차이 (2026-02-28)

### 증상
- Anthropic의 새로운 **automatic caching** (top-level `cache_control`)을 테스트
- 턴 1-11까지는 시스템 프롬프트(4686 토큰) 캐시 읽기 작동
- **턴 12부터 cache_read가 완전히 0**으로 떨어짐
- 결과: 캐시 미적용 대비 **-15% (오히려 비용 증가)**

```
Turn 11: cache_read=4686  cache_create=3496   hit_rate=57.3%
Turn 12: cache_read=0     cache_create=8517   hit_rate=0.0%  ← 갑자기 0
Turn 50: cache_read=0     cache_create=21061  hit_rate=0.0%
```

### 원인

Anthropic automatic caching의 **20-block lookback 제한**:

```
자동 캐싱 = 마지막 메시지에서 역방향으로 최대 20블록만 검사

턴 12 시점: system(1) + user(11) + assistant(11) + user(1) = 24 메시지
→ 시스템 프롬프트가 마지막에서 24번째 = lookback 20 범위 밖!
→ 캐시 미스 → 매 턴 전체를 cache_create (20000+ 토큰)
```

반면 **수동 3-BP**는 system, 중간 assistant, 마지막 assistant에 **명시적 breakpoint**를 배치하므로, 20-block 제한과 무관하게 항상 캐시 히트.

### 연속 실행 비교 결과 (동일 세션, back-to-back)

3-BP 50턴 → 자동 캐싱 42턴(턴 43에서 529 Overloaded 에러로 중단)을 연속 실행하여 비교.

| 지표 | 수동 3-BP (50턴) | 자동 top-level (42턴) |
|------|:----------------:|:---------------------:|
| 평균 히트율 (턴 5+) | **95.5%** | 12.1% |
| 총 비용 | **$0.17** | $0.62 |
| 캐시 미적용 대비 비용 | $0.72 | $0.55 |
| 절감률 | **76.0%** | **-11.4% (더 비쌈)** |

**자동 캐싱의 추가 문제**: 턴 43에서 Anthropic API가 `529 Overloaded` 에러를 반환하며 중단됨. 자동 캐싱은 매 턴 전체 대화를 `cache_create`하므로 (18000+ 토큰/턴), API 처리량 한계에 도달할 수 있음.

### 교훈

1. **장기 RP 대화(20턴+)에서는 수동 3-BP가 필수** — 자동 캐싱은 짧은 대화에만 유효
2. **20-block lookback 제한**을 반드시 인지할 것 — 대화 메시지가 20개를 넘으면 자동 캐싱의 시스템 프롬프트 히트가 불가능
3. **자동 캐싱은 비용 증가 + API 불안정** — cache_create 프리미엄(25%)이 매 턴 전체에 적용되어 캐시 없는 경우보다 비쌈. 또한 대량 cache_create로 529 Overloaded 발생 가능
4. **SAGA의 3-BP 설계가 정확히 맞는 전략** — 명시적 breakpoint로 시스템/중간/마지막 지점을 고정하므로 대화 길이에 무관

### 주의: 벤치마크 스토리 품질 차이

트레이스를 비교하면 Auto-cache 쪽 서사가 더 풍부해 보인다 (void-amulet 20턴 연속 참조, Shadow Guild 9턴 등장 vs 3-BP는 반복 수렴). **이것은 캐싱 메커니즘의 차이가 아니라 확률적 분기(stochastic divergence)** 때문:

- 두 모드는 **동일한 메시지 내용**을 API에 전송 (메시지 수, 텍스트 길이 모두 일치)
- Turn 1에서 temperature=0.7로 서로 다른 초기 응답 → 이후 대화 히스토리가 완전히 갈라짐
- 3-BP 런은 10개 순환 입력에 대해 모델이 "고정점"에 수렴 (Turn 21==31 바이트 동일, 총 5쌍)
- Auto-cache 런은 우연히 더 풍부한 서사 궤적을 생성

캐싱 breakpoint는 API 백엔드 최적화일 뿐 모델 출력에 영향을 주지 않으므로, **비용/토큰 벤치마크 결과는 유효**하다. 스토리 품질은 벤치마크의 측정 대상이 아니며, 실제 SAGA에서는 유저가 매 턴 다른 입력을 하므로 순환 수렴 문제가 발생하지 않는다.

### 벤치마크 스크립트
- `tests/bench_prompt_caching.py` — 3-BP / 자동 / no-cache 모드 지원, `--trace` 플래그로 전체 I/O 트레이싱 가능
- `tests/bench_report_3bp.{txt,json}` — 수동 3-BP 50턴 데이터
- `tests/bench_report_auto.{txt,json}` — 자동 캐싱 42턴 데이터 (턴 43 529 에러로 중단)
- `tests/bench_trace_3bp.json` — 수동 3-BP 전체 I/O 트레이스
- `tests/bench_trace_auto.json` — 자동 캐싱 전체 I/O 트레이스
