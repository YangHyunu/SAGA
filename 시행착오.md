# SAGA 시행착오 기록

## 1. Anthropic Prompt Caching 무효화 버그 (2026-02-25)

### 증상
- `cache_read`가 매 턴 5553 토큰에서 고정
- `cache_create`가 매 턴 14000~15000 토큰씩 새로 생성
- Turn 2 이후에도 이전 턴의 캐시가 전혀 재사용되지 않음

```
Turn 1: input=24  cache_read=5553  cache_create=14187
Turn 2: input=9   cache_read=5553  cache_create=14892  ← 14187이 재사용 안됨
Turn 3: input=25  cache_read=5553  cache_create=15573  ← 계속 새로 생성
```

### 원인

Anthropic API의 prompt caching은 **prefix 기반**이다. 즉 요청의 처음부터 cache breakpoint(BP)까지의 내용이 동일해야 캐시가 hit된다.

`server.py`의 `_build_cacheable_messages()`에서 동적 컨텍스트(에피소드, 로어, 상태 등)를 `role: "system"` 메시지로 대화 중간에 삽입했다:

```python
# 문제의 코드
messages.insert(last_user_idx, {
    "role": "system",        # ← 이게 문제
    "content": context_block, # 매 턴 내용이 바뀜
})
```

그런데 `llm/client.py`의 `_call_anthropic()`에서는 **모든 system 메시지를 `body["system"]` 배열로 호이스팅**한다:

```python
for msg in messages:
    if msg["role"] == "system":
        system_parts.append(part)  # 전부 system 배열로 올림
    else:
        non_system.append(entry)
```

결과적으로 Anthropic API에 전송되는 구조:

```
system: [
  시스템프롬프트 (BP1, 고정, 5553 토큰),     ← cache hit OK
  동적컨텍스트 (매 턴 변경, ~1500 토큰)       ← 이것 때문에 BP2 prefix 변경!
]
messages: [
  user1,
  assistant1 (BP2),   ← prefix에 동적컨텍스트 포함 → 캐시 무효화
  user2
]
```

Anthropic caching에서 BP2의 캐시 키 = `system[0] + system[1] + user1 + assistant1`인데, `system[1]`(동적컨텍스트)이 매 턴 바뀌므로 BP2 캐시가 **절대로 hit되지 않는다**.

5553 토큰의 `cache_read`는 BP1(시스템프롬프트)만 히트된 것이고, 나머지 14000+ 토큰은 매번 새로 캐시를 생성하고 있었다.

### 해결

동적 컨텍스트를 system 메시지가 아닌 **마지막 user 메시지에 prepend**하도록 변경:

```python
# 수정 후
if last_user_idx is not None:
    messages[last_user_idx] = dict(messages[last_user_idx])
    messages[last_user_idx]["content"] = context_block + "\n\n" + messages[last_user_idx]["content"]
```

수정 후 API 구조:

```
system: [
  시스템프롬프트 (BP1, 고정)         ← cache hit
]
messages: [
  user1,                            ← 고정
  assistant1 (BP2),                 ← prefix 고정 → cache hit!
  "동적컨텍스트\n\nuser2"           ← BP 뒤에 위치, 캐시에 영향 없음
]
```

### 수정 후 결과

```
Turn 1: input=317   cache_read=5,553   cache_create=15,719  (초기 캐시 생성)
Turn 2: input=589   cache_read=21,272  cache_create=441     ← 95% 캐시 히트!
Turn 3: input=809   cache_read=21,713  cache_create=522     ← 96% 캐시 히트!
Turn 4: input=1,006  cache_read=22,235  cache_create=456     ← 96% 캐시 히트!
```

Turn 2부터 이전 턴의 `cache_create`가 다음 턴의 `cache_read`로 전환되어 **~96% 캐시 히트율** 달성.

### 교훈

1. **Anthropic API는 모든 system 메시지를 messages 앞의 system 배열로 호이스팅한다** — 대화 중간에 삽입한 system 메시지도 앞으로 올라간다
2. **Prompt caching은 prefix 기반** — cache breakpoint 이전의 내용이 1 토큰이라도 바뀌면 해당 BP 이후 전체 캐시가 무효화된다
3. **동적 컨텍스트는 반드시 모든 cache breakpoint 뒤에 배치**해야 한다 — user 메시지에 prepend하는 것이 가장 안전한 방법
4. `cache_read`가 고정되어 있고 `cache_create`가 매 턴 비슷한 크기라면 캐시가 무효화되고 있다는 신호

### 수정 파일
- `saga/server.py` → `_build_cacheable_messages()` (lines 286~312)
