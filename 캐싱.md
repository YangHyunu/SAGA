# SAGA Prompt Caching 설계 가이드

## 1. Anthropic Prompt Caching 기본 원리

### 캐시 접두사 계층

캐시는 `tools → system → messages` 순서로 누적 해싱된다. 각 수준의 변경은 해당 수준 + 모든 후속 수준을 무효화한다.

```
[tools] → [system] → [messages...]
   ↓          ↓           ↓
 변경 시    변경 시      변경 시
 전체 무효  system+msg   msg만 무효
```

| 변경 사항 | tools 캐시 | system 캐시 | messages 캐시 |
|-----------|:----------:|:-----------:|:-------------:|
| 도구 정의 변경 | X | X | X |
| system prompt 변경 | O | X | X |
| 메시지 내용 변경 | O | O | X (해당 위치 이후) |

### 역방향 20블록 탐색 창

명시적 `cache_control` 중단점(BP)에서 **역방향으로 최대 20블록**만 확인한다.

```
블록30에 BP 설정, 31개 블록 전송:

Case 1: 변경 없음
  BP(30) → 확인 → 히트! → 블록31만 처리

Case 2: 블록25 수정
  BP(30) → 29 → 28 → 27 → 26 → 25(불일치) → 24(히트!)
  → 블록 25~30 재처리

Case 3: 블록5 수정 (BP 없음)
  BP(30) → 29 → ... → 11 (20번째) → 탐색 중단!
  → 전체 재처리 (블록5가 20블록 창 밖)

Case 4: 블록5 수정 + 블록5에 BP 추가
  BP(30) → ... → 11 (중단) → BP(5)로 이동 → 4(히트!)
  → 블록 5~30만 재처리
```

**핵심: BP는 20블록 창의 "다리" 역할** — 대화가 길어져도 BP 간격이 20블록 이내면 전체 캐시 히트 가능.

### 토큰 분류

```
total_input = cache_read + cache_creation + input_tokens
```

| 필드 | 의미 | 비용 (Haiku 4.5) |
|------|------|:----------------:|
| `cache_read_input_tokens` | BP 이전, 이미 캐시됨 → 읽기 | $0.10/MTok (90% 할인) |
| `cache_creation_input_tokens` | BP 이전, 새로 캐시 생성 | $1.25/MTok (25% 프리미엄) |
| `input_tokens` | 마지막 BP 이후 토큰 (캐시 대상 아님) | $1.00/MTok (정가) |

캐싱이 이득이 되려면: `cache_read × 0.10 + cache_create × 1.25 < total × 1.00`
→ **cache_read 비율이 높아야** 절감됨. cache_create만 계속 발생하면 오히려 비용 증가.

### 모델별 최소 캐시 토큰

| 모델 | 최소 캐시 가능 토큰 |
|------|:-----------------:|
| Claude Opus 4.6 / 4.5 | 4,096 |
| Claude Sonnet 4.6 | 2,048 |
| Claude Sonnet 4.5 / 4 / 3.7 | 1,024 |
| **Claude Haiku 4.5** | **4,096** |
| Claude Haiku 3.5 / 3 | 2,048 |

임계값 미만이면 `cache_control`을 달아도 **에러 없이 조용히 무시**됨. `cache_creation: 0`이면 프롬프트 길이를 의심할 것.

### 캐시 TTL

- 5분 TTL (마지막 사용 기준)
- 사용될 때마다 리셋됨
- 5분 이내 재요청 시 cache_read 가능

---

## 2. SAGA 3-BP 전략

### 구조

```
[system prompt BP1] → [user1] → [asst1] → ... → [asst_mid BP2] → ... → [asst_last BP3] → [동적컨텍스트 + user_input]
 ━━━ 절대 안 변함 ━━━   ━━━━━━ 이전 턴 = 안 변함 ━━━━━━                                    ━━ 매 턴 변함 ━━
```

| BP | 위치 | 역할 | 변경 빈도 |
|----|------|------|----------|
| **BP1** | system prompt | 세계관, 룰, 캐릭터 설정 (~14K tok) | 세션 내 불변 |
| **BP2** | 대화 중간 assistant | 긴 대화에서 앞부분 캐시 | 새 턴이 추가될 때만 위치 이동 |
| **BP3** | 마지막 assistant | 직전 턴까지 캐시 | 매 턴 마지막 위치가 이동 |

### 왜 3개인가?

1. **BP1 (system)**: 항상 히트. 14K 토큰을 매 턴 90% 할인으로 읽음.
2. **BP2 (mid)**: 대화가 40블록 이상일 때 BP1↔BP3 간격이 20블록을 초과하는 것을 방지. "다리" 역할.
3. **BP3 (last assistant)**: 직전 턴까지의 대화를 캐시. 새 턴에서 이전 assistant까지 cache_read → 새 user만 input_tokens.

### 동적 컨텍스트 배치 규칙

```
❌ 잘못된 배치 (시행착오 #1에서 발견):
system: [시스템프롬프트 BP1, 동적컨텍스트]  ← 매 턴 system 변경 → BP2/BP3 전부 무효화!

✅ 올바른 배치:
system: [시스템프롬프트 BP1]
messages: [...BP2...BP3..., "동적컨텍스트\n\n유저입력"]  ← 모든 BP 뒤에 위치
```

**왜?** `_call_anthropic()`이 모든 `role: "system"` 메시지를 `body["system"]` 배열로 호이스팅한다. 대화 중간에 삽입한 system 메시지도 앞으로 올라가서 BP1 뒤의 prefix를 변경 → 캐시 무효화.

**해결**: 동적 컨텍스트를 마지막 user 메시지에 prepend. 모든 BP 뒤에 위치하므로 캐시에 영향 없음.

### 코드 위치

```python
# saga/server.py: _build_cacheable_messages() (line 256~)

# BP1: system prompt
messages[system_idx]["cache_control"] = {"type": "ephemeral"}     # line 275

# BP2: 중간 assistant (대화가 2+ assistant일 때)
mid_idx = assistant_indices[len(assistant_indices) // 2]
messages[mid_idx]["cache_control"] = {"type": "ephemeral"}        # line 286

# BP3: 마지막 assistant
last_idx = assistant_indices[-1]
messages[last_idx]["cache_control"] = {"type": "ephemeral"}       # line 291

# 동적 컨텍스트: 마지막 user에 prepend (line 316~318)
messages[last_user_idx]["content"] = context_block + "\n\n" + messages[last_user_idx]["content"]
```

---

## 3. 자동 캐싱이 실패하는 이유

### 자동 캐싱 (top-level `cache_control`)

```python
body["cache_control"] = {"type": "ephemeral"}  # 요청 본문 최상위
```

시스템이 마지막 블록에서 역방향 20블록만 탐색:

```
턴 12: system(1) + user(11) + assistant(11) + user(1) = 24 메시지
역방향: msg24 → msg23 → ... → msg5 (20번째) → 탐색 중단!
system prompt = msg1 = 탐색 범위 밖 → 캐시 미스
→ 매 턴 전체를 cache_create (18,000+ 토큰 × $1.25/MTok)
```

### 벤치마크 비교 (50턴, Claude Haiku 4.5)

| | 수동 3-BP | 자동 top-level |
|---|:---:|:---:|
| 턴 5 히트율 | 91.8% | 75.7% |
| 턴 12 히트율 | 93.7% | **0.0%** ← 20블록 초과 |
| 턴 50 히트율 | 97.3% | 0.0% |
| 평균 히트율 (턴 5+) | **95.5%** | 12.1% |
| 총 비용 | **$0.17** | $0.62 |
| 캐시 미적용 대비 | **76.0% 절감** | **11.4% 추가** |

자동 캐싱은 턴 43에서 529 Overloaded 에러로 중단됨 — 매 턴 대량 cache_create가 API 처리량 한계 도달.

---

## 4. 턴별 토큰 흐름 (3-BP)

### Turn 1 (Cold Start)

```
cache_read:      0  ← 첫 요청, 캐시 없음
cache_create: 4,686  ← BP1(system prompt) 캐시 생성
input_tokens:   178  ← 동적 컨텍스트 + user 입력
output_tokens:  300
```

### Turn 2

```
cache_read:   4,686  ← BP1 캐시 히트! (Turn 1에서 생성)
cache_create:   328  ← 새 assistant 응답 (BP2 또는 BP3)
input_tokens:   184
→ 히트율: 4,686 / (4,686 + 328 + 184) = 90.2%
```

### Turn 50

```
cache_read:  20,497  ← BP1 + 49턴 대화 전체 캐시 히트
cache_create:   328  ← Turn 49 assistant만 새로 캐시
input_tokens:   236  ← 동적 컨텍스트(~150) + user 입력(~80)
→ 히트율: 20,497 / 21,061 = 97.3%
```

### 비용 계산 (Turn 50)

```
캐시 적용:
  20,497 × $0.10/MTok = $0.00205  (cache_read, 90% 할인)
  +  328 × $1.25/MTok = $0.00041  (cache_create, 25% 프리미엄)
  +  236 × $1.00/MTok = $0.00024  (input, 정가)
  +  300 × $5.00/MTok = $0.00150  (output)
  = $0.00420/턴

캐시 미적용:
  21,061 × $1.00/MTok = $0.02106
  +  300 × $5.00/MTok = $0.00150
  = $0.02256/턴

절감: 81.4%
```

---

## 5. 실전 적용 시 체크리스트

### 캐시가 작동하는지 확인

- [ ] `cache_creation > 0` 인가? → 0이면 최소 토큰 임계값 미달 (Haiku 4.5는 4,096)
- [ ] `cache_read`가 턴 2+부터 증가하는가? → 안 증가하면 prefix가 변하고 있음
- [ ] `cache_read`가 고정이고 `cache_create`가 매 턴 비슷하면 → 캐시 무효화 신호

### 무효화 방지

- [ ] 동적 컨텍스트를 system role로 보내지 않는가?
- [ ] 동적 컨텍스트가 모든 BP 뒤에 위치하는가? (마지막 user에 prepend)
- [ ] tool 정의가 매 요청 동일한가? (변경 시 전체 무효화)
- [ ] image/파일 첨부가 매 턴 바뀌지 않는가? (messages 캐시 무효화)

### BP 간격

- [ ] BP1 ↔ BP2 간격 < 20블록인가?
- [ ] BP2 ↔ BP3 간격 < 20블록인가?
- [ ] 대화가 40턴 이상이면 BP2가 적절한 중간점에 있는가?

### 비용 모니터링

```python
# 매 턴 로깅
hit_rate = cache_read / (cache_read + cache_create + input_tokens)
cost = cache_read * 0.10e-6 + cache_create * 1.25e-6 + input * 1.00e-6 + output * 5.00e-6

if hit_rate < 0.5 and turn > 5:
    logger.warning(f"Low cache hit rate: {hit_rate:.1%}")
if cache_creation == 0 and prompt_caching_enabled:
    logger.warning("cache_creation=0: check minimum token threshold")
```

---

## 6. 시행착오 요약

| # | 문제 | 원인 | 해결 |
|---|------|------|------|
| 1 | cache_read 고정, cache_create 매 턴 증가 | 동적 컨텍스트를 system role로 삽입 → 호이스팅 → prefix 변경 | user 메시지에 prepend |
| 2 | cache_creation이 항상 0 | Haiku 4.5 최소 4,096 토큰 미달 | 시스템 프롬프트를 4,100+ 토큰으로 확장 |
| 3 | 자동 캐싱 턴 12+에서 0% 히트 | 20블록 lookback 제한 | 수동 3-BP로 명시적 BP 배치 |

상세: `시행착오.md` | 벤치마크: `tests/bench_prompt_caching.py` | 데이터: `tests/bench_report_3bp.json`
