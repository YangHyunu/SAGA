# 구조 제안 — RP 장기 세션의 상태 유실 문제를 프록시로 풀어보기

**SAGA RP Agent Proxy v3.0 — Stateful RAG 기반 Context Engineering**

---

> 단순 캐릭터챗 AI에서 Mem0, Langmem, Letta 같은 Stateful Agent는 과한 구조였습니다.
> 하지만 RP는 다릅니다 — 위치, NPC, 아이템, 관계, 세력 등 동적인 요소가 훨씬 많고,
> 유저가 직접 세계를 설계하고 만들어 나갑니다.
> 코히바블랙님의 연구일지[1]를 읽으며 이 가능성을 확인했고,
> **Letta의 판단력을 매 턴이 아닌 필요한 순간에만 쓰면 어떨까** — 여기서 출발했습니다.
> Letta를 핵심에 두되, 프록시 기반 1회 호출 + 비동기 추출로 풀어본 기록입니다.

---

## 목차

1. [우리 모두가 겪는 문제](#1-우리-모두가-겪는-문제)
2. [기존 접근들의 한계](#2-기존-접근들의-한계)
3. [Letta의 Step Loop — Stateful Agent의 구조](#3-letta의-step-loop--stateful-agent의-구조)
4. [SAGA의 아이디어: "에이전트는 큐레이션만"](#4-saga의-아이디어-에이전트는-큐레이션만)
5. [어떻게 동작하는가](#5-어떻게-동작하는가)
6. [3-Agent 파이프라인 상세](#6-3-agent-파이프라인-상세)
7. [다이나믹 로어북 — 정적 로어북의 진화](#7-다이나믹-로어북--정적-로어북의-진화)
8. [비용과 트레이드오프](#8-비용과-트레이드오프)
9. [코히바블랙님 및 선행 연구자분들께](#9-코히바블랙님-및-선행-연구자분들께)
10. [만들면서 느낀 것](#10-만들면서-느낀-것)
11. [참고 자료](#11-참고-자료)

---

## 1. 우리 모두가 겪는 문제

RP를 해본 사람이라면 이런 경험이 있을 겁니다.

> 30턴 전에 죽인 NPC가 아무렇지 않게 다시 등장한다.
> 버린 아이템이 인벤토리에 여전히 있다.
> 동쪽으로 이동했는데 서쪽 마을에서 대화가 이어진다.
> HP가 0인데 전투를 계속한다.

장기 RP 세션의 근본적인 문제는 **상태 유실**입니다. LLM은 대화가 길어지면 이전 상태를 잊어버립니다. 컨텍스트 창을 128K, 200K로 늘려도 본질은 같습니다 — 50턴 전 세부사항을 정확히 기억하리라는 보장이 없고, 비용만 선형으로 증가합니다.

그리고 또 하나의 문제: **로어북은 정적**입니다. "에르겐은 마을 광장의 약초상이다"라고 작성해 두면, 에르겐이 숲으로 이동하거나 사망하더라도 로어북은 변하지 않습니다. LLM은 오래된 설정을 보고 모순된 이야기를 만들게 됩니다.

---

## 2. 기존 접근들의 한계

이 문제를 풀려는 시도는 여러 가지가 있었습니다.

| 접근 방식 | 상태 추적 | 구조화 | 유저 지연 | 비용 | 한계 |
|-----------|:--------:|:------:|:--------:|:----:|------|
| **정적 로어북** | X | X | 없음 | 낮음 | 세계가 변해도 로어북은 고정 |
| **전체 히스토리 전송** | 암묵적 | X | 없음 | 높음 | 토큰 낭비 + 장기 세션에서 누적 모순 |
| **HypaMemory 계열** | 대화 기억 | X | 낮음 | 중간 | 요약 기반 압축 — 구조화된 상태 추적 불가 |
| **Mem0 계열** | 대화 기억 | X | **높음** | 중간 | 매 턴 메모리 추출/갱신이 동기적 → 응답 지연 |
| **SAGA (이 프로젝트)** | O | O | **낮음** | 중간 | 추출 정확도가 State Block 품질에 의존 |

**HypaMemory 계열**(RisuAI HypaMemory/SupaMemory, SillyTavern Summarize 등)은 토큰 한계에 도달하면 대화를 요약·압축하여 장기 기억을 시뮬레이션합니다. 대화의 흐름은 기억하지만, "캐릭터가 어디에 있는지", "HP가 얼마인지", "누구와 어떤 관계인지"를 **구조화된 형태로** 추적하지는 못합니다. 요약 과정에서 세부 상태가 소실되므로, 장기 세션의 모순까지 막기엔 부족합니다.

**Mem0 계열**은 LLM이 직접 메모리를 관리하지는 않지만, 매 대화마다 메모리 추출과 갱신이 동기적으로 실행되어 응답 경로에 지연이 발생합니다. 구조화된 상태 추적도 지원하지 않습니다.

---

## 3. Letta의 Step Loop — Stateful Agent의 구조

코히바블랙님이 선행연구[1]에서 언급하셨듯, Letta(MemGPT)[2]는 LLM에 가상 메모리 계층을 부여하여 **진짜 Stateful Agent**를 만드는 프레임워크입니다. 에이전트가 자신의 Memory Block을 직접 읽고 편집하면서 상태를 유지합니다.

핵심은 **Step Loop** 구조입니다. 에이전트가 한 턴에 여러 단계를 밟으며, 각 단계에서 "무엇을 읽을지", "어떻게 수정할지"를 LLM이 판단합니다:

```
유저: "마을 광장을 둘러본다"

[Step 1] 내면 사고 → "기억을 확인해야겠다"       ← LLM 호출
[Step 2] memory_read(core_memory)              ← 도구 실행
[Step 3] 내면 사고 → "위치가 바뀌었으니 수정"     ← LLM 호출
[Step 4] memory_edit(core_memory, location=...) ← 도구 실행
[Step 5] 내면 사고 → "archival도 확인"           ← LLM 호출
[Step 6] archival_search("마을 광장")            ← 도구 실행
[Step 7] 내면 사고 → "응답 생성"                 ← LLM 호출
[Step 8] send_message("마을 광장은...")          ← 최종 응답

= 4회 LLM 호출
```

이 구조의 강점은 **유연성**입니다. 에이전트가 매 턴 상황을 판단하고, 필요한 기억만 선택적으로 읽고, 적절하게 편집합니다. 예상치 못한 상황에도 에이전트가 스스로 대응할 수 있습니다.

대신 **메모리 읽기/쓰기 자체가 LLM 호출**이므로, 턴당 3~5회의 호출이 발생합니다. 이게 Stateful의 대가입니다 — 생각하고, 도구 쓰고, 또 생각하는 과정이 매 턴 반복됩니다.

SAGA는 이 Letta의 Step Loop를 핵심에 활용하되, **"매 턴 다회 호출"을 "N턴마다 비동기"로 재배치**하는 아이디어에서 출발했습니다.

---

## 4. SAGA의 아이디어: "에이전트는 큐레이션만"

### 코딩 에이전트에서 얻은 힌트

혹시 Claude Code, OpenClaw, oh-my-claudecode(OMC) 같은 코딩 에이전트를 써보셨나요? 이 도구들은 멀티 에이전트 오케스트레이션, 복잡한 워크플로우 관리, 스킬 체이닝 등을 잘 해냅니다. 그런데 실제로 써보면 의외의 지점을 발견합니다 — 컨텍스트 관리의 핵심은 정교한 메모리 시스템이 아니라 **.md 형식의 구조화된 문서**였습니다. 이 에이전트들은 장기 작업 중에 핵심 정보를 **Notepad/NOTE** 형태로 자기관리합니다. 에이전트가 "무엇이 중요한지" 스스로 판단하여 메모를 갱신하는 패턴입니다.

여기서 한 가지 더 중요한 점이 있습니다. 이 도구들은 **모든 것을 직접 만들지 않습니다.** Claude Code는 Bash, grep, git 같은 기존 도구를 오케스트레이션하고, OMC는 여러 에이전트를 조율하며, OpenClaw는 기존 LLM API를 게이트웨이로 중계합니다. 핵심은 **이미 잘 만들어진 것들을 어떻게 연결하고 관리하느냐**입니다.

SAGA도 같은 철학입니다. Letta(Stateful Agent), ChromaDB(벡터 검색), Kuzu(그래프 DB), SQLite(KV 저장소) — 각각은 이미 검증된 도구입니다. SAGA가 하는 건 이것들을 **RP에 맞는 하나의 파이프라인으로 엮고, 언제 무엇을 호출할지 관리하는 오케스트레이션**입니다. 새로운 DB나 메모리 시스템을 만드는 게 아닙니다.

이걸 RP에 적용할 때 핵심 질문은 이것이었습니다:

> **에이전트의 판단력이 정말 필요한 곳은 어디인가?**

생각해 보면:

- **매 턴 상태 추출** — "위치가 바뀌었는가? HP가 변했는가?" → 정규식이나 저비용 LLM으로 충분합니다. 에이전트가 고민할 필요 없습니다.
- **매 턴 컨텍스트 조립** — "어떤 로어북이 관련 있는가?" → 점수 기반 필터링으로 충분합니다.
- **N턴마다 서사 큐레이션** — "이 모순은 의도된 것인가? 이 복선은 회수해야 하는가?" → **여기서만 에이전트의 판단력이 필수입니다.** 이전 큐레이션 결정을 기억하고 일관되게 이어가야 합니다.

### .md — 에이전트 메모리의 사실상 표준

그런데 이런 컨텍스트 관리 패턴을 들여다보면, 공통점이 있습니다. 아키텍처는 전부 다른데, **최종 형식은 다 마크다운**입니다.

| 시스템 | 아키텍처 | 컨텍스트 관리 |
|--------|---------|-------------|
| **Claude Code** | CLI 에이전트 | `CLAUDE.md`, `~/.claude/memory/*.md` |
| **OMC** | 오케스트레이터 | `notepad.md`, `AGENTS.md`, `.omc/plans/*.md` |
| **OpenClaw** | Gateway/프록시 | `.md` 기반 컨텍스트 주입 |
| **Codex** | CLI 에이전트 | `AGENTS.md`, 마크다운 기반 지시 |
| **Letta Code** | 에이전트 프레임워크 | `memory/*.md` + YAML frontmatter (MemFS)[3] |
| **SAGA** | Gateway/프록시 | `stable_prefix.md`, `live_state.md` |

Swarm이든, Orchestrator든, Skill 시스템이든 — 상위 구조는 다 달라도 에이전트에게 컨텍스트를 전달하는 최종 형식은 마크다운으로 수렴하고 있습니다. 왜일까요?

- **LLM이 가장 잘 읽는 형식**: 학습 데이터에 마크다운이 압도적으로 많음
- **구조화와 가독성의 균형**: JSON은 기계적이고 자연어는 비구조적인데, 마크다운은 헤딩/리스트/테이블로 반구조화 가능
- **YAML frontmatter로 메타데이터 분리**: 본문은 LLM이 읽고, 프론트매터는 코드가 읽음
- **diff/캐싱 친화적**: 텍스트 기반이라 git diff, Prompt Caching 모두 효율적

SAGA의 .md 캐시 2파일은 이 흐름을 RP에 적용한 것이고, 그 중에서도 Letta Code의 MemFS가 가장 직접적인 참고점이었습니다.

### 그래서 Letta는 Curator 전담으로

Letta의 Step Loop가 강력한 건 맞지만, 매 턴 돌리기엔 비용이 큽니다. 그렇다면 **Letta를 매 턴이 아닌, 서사 큐레이션에만 집중시키자** — 여기서 SAGA가 시작됩니다.

Curator는 Letta SDK로 생성된 에이전트가 Memory Block(`narrative_memory`)을 자기편집하면서 서사 판단의 연속성을 유지합니다. 10턴 전에 "이 모순은 의도된 전개"라고 기록해 두면, 20턴 후에도 그 판단을 이어갈 수 있습니다. 이건 Letta의 Memory Block 없이는 불가능합니다.

그리고 Letta가 최근 진화시킨 **MemFS(Memory File System)**[3] 구조도 알고 있었습니다. Letta Code는 에이전트 메모리를 **마크다운 파일 시스템**으로 관리합니다:

```
memory/
  project_overview.md      ← YAML frontmatter + 마크다운 본문
  architecture_notes.md
  current_tasks.md
  ...
```

각 파일에 YAML frontmatter가 붙고, 에이전트가 `create_file`, `modify_file` 같은 도구로 자기편집합니다. git이 버전 관리를 담당합니다.

SAGA의 **.md 캐시 2파일** 설계는 이 구조에서 직접 영향을 받았습니다:

| | Letta MemFS | SAGA .md 캐시 |
|---|---|---|
| 형식 | 마크다운 + YAML frontmatter | 마크다운 + YAML frontmatter |
| 파일 구조 | `memory/project_overview.md` 등 | `stable_prefix.md`, `live_state.md` |
| 버전 관리 | git (커밋 기반) | YAML frontmatter의 `updated_at`, `turn` 필드 |
| 편집 주체 | **에이전트** (`modify_file` 도구) | **코드** (정규식 파싱 → `os.replace()` 원자적 교체) |
| 편집 비용 | LLM 호출 필요 | LLM 호출 불필요 |

차이는 **"누가 편집하는가"** 입니다. Letta MemFS는 에이전트가 파일을 읽고 판단하고 편집합니다. SAGA는 같은 구조를 쓰되, **매 턴 편집은 코드 로직(Sub-B)에게 위임**합니다. 대신 **N턴마다 Letta Curator가 .md 캐시를 큐레이션**합니다 — `live_state.md` 압축, 모순 수정 등.

> 정리하면: Letta의 Memory Block은 Curator의 두뇌이고,
> Letta의 MemFS 패턴은 .md 캐시의 설계 원형입니다.
> SAGA가 바꾼 건 **"매 턴 편집을 에이전트가 아닌 코드가 한다"** 는 것뿐입니다.

### 그래서 이렇게 분리했습니다

```
┌─────────────────────────────────────────────────────┐
│  메인 루프 (매 턴, 유저 대기)                         │
│                                                     │
│  Sub-A: DB 검색 + 프롬프트 조립   ← 코드 로직, LLM 0회  │
│  LLM: 내레이션 1회 호출            ← 이것만 유저 대기    │
│  Sub-B: 상태 추출 + DB 갱신       ← 비동기, 유저 무관   │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│  Curator (10턴마다, 비동기)                           │
│                                                     │
│  Letta Memory Block 자기편집      ← 에이전트 판단 필요  │
│  모순 탐지, 서사 압축, 이벤트 관리   ← 다회 호출 OK     │
│                                    (유저 대기 아님)    │
└─────────────────────────────────────────────────────┘
```

코딩 에이전트가 Notepad를 통해 작업 맥락을 자기관리하듯, **Letta Curator는 Memory Block을 통해 큐레이션 판단 이력을 자기관리**합니다. 단, 이 과정은 10턴마다 비동기로만 일어나므로 유저가 기다릴 일이 없습니다.

---

## 5. 어떻게 동작하는가

SAGA는 **OpenAI-compatible 프록시**입니다. 클라이언트(RisuAI, SillyTavern 등)와 LLM 사이에 끼어서, 프론트엔드 수정 없이 동작합니다. API Base URL만 바꾸면 됩니다.

### Read-Write 순환

매 턴은 읽기(READ)와 쓰기(WRITE)의 순환입니다:

```
                    ┌──────────────────────────────┐
                    │         2종 DB 저장소           │
                    │  SQLite(상태) + ChromaDB(벡터)  │
                    └──────┬───────────────┬────────┘
                   READ    │               │  WRITE
                (동기 ~35ms)│               │(비동기)
                           ▼               │
    ┌────────┐    ┌────────────────┐    ┌──┴──────────────┐
    │ Client │───▶│  Sub-A:        │───▶│  LLM (1회 호출)   │
    │        │◀───│  Context Build │◀───│                  │
    └────────┘    └────────────────┘    └──┬──────────────┘
                                           │ 응답 반환 후
                                           ▼
                                    ┌──────────────┐
                                    │  Sub-B:      │
                                    │  추출 + 갱신  │──▶ DB WRITE
                                    └──────────────┘
```

1. **READ** (동기, ~35ms): Sub-A가 2종 DB에서 현재 상태, 관련 로어북을 검색 → 프롬프트에 주입
2. LLM 1회 호출 → 유저에게 응답 반환
3. **WRITE** (비동기, 유저 대기 없음): Sub-B가 응답에서 상태 변화를 추출 → 2종 DB 갱신

이 순환이 매 턴 반복되므로, DB는 항상 최신 세계 상태를 반영합니다.

### 왜 2종 DB인가?

Letta는 에이전트의 메모리를 Core / Archival / Recall 3계층으로 나눕니다. SAGA는 이 구조를 에이전트 호출 없이 코드가 직접 접근하는 2종 DB로 대체했습니다.

| Letta 메모리 계층 | SAGA DB | 역할 | 예시 쿼리 |
|---|---|---|---|
| **Core Memory** (항상 로드) | **SQLite** | 현재 상태, 세션 메타, **캐릭터, 관계, 장소, 이벤트, 로어** | "플레이어 위치는?" "에르겐이 아는 사람은?" |
| **Archival Memory** (임베딩 검색) | **ChromaDB** (벡터) | 로어북, 에피소드 기억 | "어둠의 숲과 비슷한 장소는?" |

그리고 이 두 DB 위에 **.md 캐시 2파일**이 있습니다 — `stable_prefix.md`, `live_state.md`. 이 파일들은 프롬프트의 캐싱 가능한 프리픽스로 사용되어, Anthropic의 Prompt Caching과 결합하면 비용을 크게 줄입니다.

### SQLite 관계 테이블

SAGA는 그래프 DB 대신 SQLite 테이블로 엔티티와 관계를 관리합니다 (예시입니다. 확장할 수 있습니다):

| 테이블 | 주요 컬럼 | 역할 |
|---|---|---|
| `characters` | name, location, hp, status | 캐릭터 현재 상태 |
| `relationships` | from_char, to_char, rel_type, strength | 캐릭터 간 관계 |
| `locations` | name, description, adjacent | 장소 및 인접 정보 |
| `events` | turn, description, participants | 이벤트 이력 |
| `lore` | keyword, content, category | 로어북 항목 |

"아리아가 어둠의 숲에서 고블린왕 크룩을 만났다"는 것이 SQLite에서는:
- `characters`: 아리아 → `location = 어둠의 숲`
- `relationships`: (아리아, 크룩, met, -2)
- `events`: Turn N | 아리아가 크룩과 조우

로 기록됩니다. "근처에 누가 있지?"는 `characters` 테이블 위치 필터 쿼리로 즉시 답할 수 있습니다.

### 에피소드 검색: 3-stage ChromaDB + SQLite

ChromaDB 벡터 검색과 SQLite 구조 쿼리를 조합하여 관련 컨텍스트를 수집합니다:

```
ChromaDB 3-stage 에피소드 검색:
    ├─ Recent:    최근 N턴 에피소드 (시간 순서 보장)
    ├─ Important: 중요도 점수 상위 에피소드
    └─ Similar:   유저 입력 기반 벡터 유사도 검색

SQLite 로어 조회:
    ├─ characters: 현재 위치, HP, 상태
    ├─ relationships: 관련 캐릭터 간 관계
    └─ lore: 키워드 매칭 로어북 항목

→ 병합 + 토큰 예산 내 우선순위 정렬
```

벡터가 "비슷한 에피소드"를 찾아주고, SQLite가 "현재 상태와 관계"를 구조적으로 제공합니다.

---

## 6. 3-Agent 파이프라인 상세

### Sub-A: Context Builder (동기, ~35ms, LLM 호출 없음)

유저 요청이 올 때마다 실행됩니다. **LLM을 호출하지 않으므로** 매우 빠릅니다.

```
stable_prefix.md 읽기 → live_state.md 읽기 → ChromaDB 3-stage 에피소드 검색
→ SQLite 로어 조회 + ChromaDB 벡터 검색 → 토큰 예산 내 조립
```

조립된 컨텍스트는 시스템 메시지에 주입됩니다:

```
[--- SAGA Dynamic Context ---]
(stable_prefix.md)

[최신 변경]
위치: 어둠의 숲 | HP: 85/100 | 인벤토리: 불꽃 검, 치유 물약

[관련 로어북]
- 어둠의 숲: 에르시아 변방의 위험한 숲...

[--- SAGA State Tracking ---]
응답 마지막에 상태 블록을 추가해주세요: (state block 형식 지시)
```

**토큰 예산**: 동적 컨텍스트에는 설정 가능한 토큰 예산이 적용됩니다. .md 캐시, 로어북, 그래프 확장, 상태 브리핑, 지시문 각각에 할당량이 있고, 예산 안에서 관련성 높은 정보를 우선 패킹합니다.

### Sub-B: Post-Turn Extractor (비동기, 유저 대기 없음)

응답이 유저에게 전달된 **뒤에** `asyncio.create_task`로 백그라운드 실행됩니다.

1. LLM 응답에서 **State Block 파싱** (정규식 → 실패 시 경량 LLM으로 폴백 추출)
2. SQLite 상태 테이블 갱신 (characters, relationships, locations, events)
3. SQLite 턴 로그 기록
4. ChromaDB 에피소드 기록 ("Turn 5 | 어둠의 숲 | 만남: 고블린 족장")
5. live_state.md 원자적 갱신 (.tmp → `os.replace()` 원자적 교체)

**State Block**은 LLM에게 응답 끝에 출력하도록 요청하는 구조화된 블록입니다:

````
```state
location: 어둠의 숲
location_moved: true
hp_change: -15
items_gained: [불꽃 검]
npc_met: [고블린왕 크룩]
relationship_changes: [{from: 아리아, to: 크룩, type: hostile, delta: -2}]
mood: tense
```
````

이걸 파싱해서 DB를 갱신합니다. 유저에게는 State Block이 제거된 깨끗한 응답만 전달됩니다.

### Curator (10턴마다, 비동기)

여기서 **Letta가 등장**합니다.

Curator는 Letta SDK로 생성된 에이전트가 Memory Block(`narrative_memory`)을 가지고 동작합니다. 10턴마다 호출되어:

- **서사 모순 탐지**: 죽은 NPC가 살아있는지, HP 0인데 생존인지, 위치 불일치가 있는지
- **장기 서사 압축**: story.md가 50턴 이상이면 요약 압축
- **이벤트 스케줄링**: 복선 회수, 새 이벤트 제안

핵심은 Letta 에이전트의 **Memory Block이 큐레이션 간에 유지**된다는 것입니다. 10턴 전에 "에르겐 위치 모순 발견"이라고 기록해 두면, 20턴 후 "그 모순은 수정되었는가?"를 이어서 판단할 수 있습니다. Direct LLM 폴백은 이 연속성이 없습니다.

Letta가 실패하면 Direct LLM으로 자동 폴백하므로, **Letta 없이도 시스템은 동작**합니다 — 다만 큐레이션 판단의 연속성이 떨어집니다.

---

## 7. 다이나믹 로어북 — 정적 로어북의 진화

전통적 로어북의 최대 문제는 **"한번 작성하면 영원히 고정"** 이라는 점이었습니다. "에르겐은 마을 광장의 약초상이다"라고 적어 두면, 에르겐이 숲으로 도망치든, 죽든, 로어북은 변하지 않습니다. SAGA는 **엔트리 자동 갱신 + 레이어 감쇠 + 점수 기반 필터링**으로 이 문제를 풀었습니다.

### 엔트리 자동 갱신

로어북 엔트리는 Sub-B가 매 턴 자동으로 갱신합니다. State Block에서 상태 변화를 추출한 뒤, 해당 엔트리의 내용과 메타데이터를 함께 업데이트합니다.

예를 들어, "에르겐이 숲으로 도망쳤다"가 발생하면:

```
[이전] 에르겐 — "마을 광장의 약초상. 조용한 성격."
                 위치: 마을 광장 | 상태: 활성 | 레이어: A2

[이후] 에르겐 — "약초상. 마을 광장에서 도주하여 현재 어둠의 숲에 은신 중."
                 위치: 어둠의 숲 | 상태: 활성 | 레이어: A2 | last_mentioned: Turn 15
```

엔트리 본문은 LLM이 수정하는 게 아니라, Sub-B의 코드 로직이 State Block의 위치/상태 변화를 파싱해서 기계적으로 반영합니다. NPC 사망, 아이템 소실, 장소 상태 변경 등이 모두 이 방식으로 처리됩니다.

### 레이어 시스템과 감쇠

모든 엔트리는 중요도에 따라 4개 레이어 중 하나에 배치됩니다:

| 레이어 | 감쇠 | 용도 | 예시 |
|:------:|:----:|------|------|
| **A1** | 없음 | 핵심 세계관, 주요 장소, 역사 | 어둠의 숲, 고대 열쇠, 대붕괴 |
| **A2** | 없음 | 주요 아이템, 활성 장소 | 마을 광장, 불꽃 검 |
| **A3** | 7턴 | 세력, 배경 설정 | 은빛 성채, 숲의 결사 |
| **A4** | 3턴 | 숨겨진 비밀, 스포일러 | 에르겐의 비밀, 고블린왕의 목적 |

A1, A2는 항상 활성입니다. A3, A4는 **감쇠 타이머**가 있어서, 일정 턴 동안 대화에서 언급되지 않으면 비활성화됩니다. 대화에서 다시 언급되면 타이머가 리셋됩니다.

이 구조의 효과:

- **핵심 세계관은 항상 참조 가능**: A1/A2는 감쇠 없이 항상 프롬프트에 포함
- **토큰 자동 절약**: 한동안 안 쓰는 배경 설정(A3)이나 비밀(A4)은 자동으로 빠짐
- **스포일러 방지**: A4 엔트리는 3턴만 유지되므로, 관련 상황에서만 잠깐 등장하고 사라짐
- **감쇠 리셋**: 대화에서 "숲의 결사"가 다시 언급되면 A3 타이머가 7턴으로 리셋

### 필터링 파이프라인

레이어 시스템만으로는 "지금 이 턴에 어떤 엔트리를 프롬프트에 넣을지"를 결정할 수 없습니다. SAGA는 벡터 검색 → 게이트 점수 → 감쇠 필터 → 레이어 가중치 → 예산 패킹의 파이프라인을 거칩니다:

```
ChromaDB 벡터 검색 결과 (10개 후보)
    │
    ▼
위치 게이트: 플레이어 현재 위치와 관련? → +3.0점
NPC 게이트: 근처 NPC와 관련?          → +2.0점
관계 전파: 알려진 관계에 포함?          → +1.0점
    │
    ▼
비활성 필터: A4 > 3턴 미언급이면 제외, A3 > 7턴 미언급이면 제외
    │
    ▼
레이어별 가중치: A1 +2.0, A2 +1.5, A3 +0.5, A4 +0.0
    │
    ▼
점수 정렬 → 토큰 예산 내에서 상위 엔트리 선택
```

### 구체적 시나리오

플레이어가 어둠의 숲에 있고, 직전 턴에 고블린왕 크룩을 만났다고 가정합니다:

| 엔트리 | 레이어 | 벡터 유사도 | 위치 게이트 | NPC 게이트 | 레이어 가중치 | **총점** |
|--------|:------:|:----------:|:----------:|:----------:|:------------:|:-------:|
| 어둠의 숲 | A1 | 0.9 | +3.0 | — | +2.0 | **5.9** |
| 고블린왕 크룩 | A2 | 0.7 | — | +2.0 | +1.5 | **4.2** |
| 대붕괴 (어둠의 숲 연관 로어) | A1 | 0.3 | — | — | +2.0 | **2.3** |
| 은빛 성채 | A3 | 0.1 | — | — | +0.5 | **0.6** |
| 에르겐의 비밀 | A4 | 0.0 | — | — | — | **제외** (4턴 미언급) |

어둠의 숲과 고블린왕 크룩이 최상위, 대붕괴가 그래프 확장으로 따라 들어오고, 은빛 성채는 점수가 낮아 토큰 예산에서 밀려납니다. 에르겐의 비밀은 A4인데 4턴째 미언급이라 아예 필터링됩니다.

이렇게 하면 **지금 이 턴에 정말 필요한 로어북만** 프롬프트에 들어갑니다.

---

## 8. 비용과 트레이드오프

### 멀티모델 전략

모든 작업에 같은 모델을 쓸 필요는 없습니다:

| 작업 | 모델 | 이유 |
|------|------|------|
| 내레이션 | Claude Sonnet 4.5 | 서사 품질이 핵심 |
| 상태 추출 (Sub-B) | 경량 LLM (Gemini Flash 등) | 구조화된 추출은 저비용 모델로 충분 |
| 큐레이션 | Claude Sonnet 4.5 | 서사 판단 필요하나, 10턴마다만 |
| 임베딩 | text-embedding-3-small | 범용, 저비용 |

### 100턴 기준 비용 비교

| 접근 방식 | LLM 호출 횟수 | 비고 |
|-----------|:-----------:|------|
| 전체 히스토리 전송 | 100 | 턴마다 히스토리 누적, 비용 선형 증가 |
| **SAGA** | **100 + 100 경량 + 10 Curator** | 내레이션 100 + 경량 LLM 추출 100 + 큐레이션 10 |

SAGA는 고비용 모델(Sonnet)은 내레이션에만, 경량 LLM은 추출에만 사용합니다. 그리고 .md 캐시 + Anthropic Prompt Caching으로 입력 토큰 비용을 추가로 줄입니다.

### Prompt Caching 실측 벤치마크 (2026-02-28)

50턴 RP 대화를 Anthropic API 직접 호출로 시뮬레이션하여, 3-BP 캐시 전략의 실전 효과를 측정했습니다 (Claude Haiku 4.5 기준).

**3-BP 수동 캐싱 (SAGA 기본 전략):**

| 구간 | 캐시 히트율 | 특이사항 |
|------|:----------:|---------|
| 턴 1 | 0% (cold) 또는 96%+ (warm) | 5분 TTL 내 이전 세션 캐시 재활용 가능 |
| 턴 2~5 | 90~92% | 시스템 프롬프트(BP1) 즉시 히트, 대화가 짧아 BP2만 활성 |
| 턴 5~25 | 92~96% | BP2(중간점) + BP3(마지막) 모두 활성, 히트율 점진 상승 |
| 턴 25~50 | 96~97% | 안정 구간, 매 턴 ~325 토큰만 새로 캐시 생성 |

**50턴 집계:**

| 지표 | 값 |
|------|-----|
| 평균 캐시 히트율 (턴 5+) | **95.5%** |
| 총 캐시 읽기 토큰 | 621,769 |
| 총 비용 (캐시 적용) | **$0.17** |
| 총 비용 (캐시 미적용) | $0.72 |
| **비용 절감** | **76.7%** |
| 평균 레이턴시 | 5,180ms (턴 수에 거의 무관) |

**자동 캐싱(top-level) vs 수동 3-BP:**

자동 캐싱은 Anthropic의 20-block lookback 제한으로 인해 턴 12+에서 캐시가 완전히 무효화됩니다. 연속 실행(3-BP 50턴 → 자동 42턴) 비교:

| 모드 | 턴 수 | 평균 히트율 | 총 비용 | 절감률 |
|------|:-----:|:----------:|:-------:|:------:|
| **수동 3-BP** | 50 | **95.5%** | **$0.17** | **76.0%** |
| 자동 top-level | 42* | 12.1% | $0.62 | -11.4% (더 비쌈) |
| 캐시 없음 | — | 0% | $0.72 | 기준선 |

*자동 캐싱은 턴 43에서 529 Overloaded 에러로 중단. 매 턴 18000+ 토큰 cache_create로 API 처리량 한계 도달.

> **결론: 장기 RP 대화에서는 수동 3-BP가 필수.** 자동 캐싱은 20턴 이상에서 오히려 비용이 증가하며, API 안정성도 떨어진다.
> 벤치마크 스크립트: `tests/bench_prompt_caching.py` (`--trace` 플래그로 I/O 트레이싱 가능) | 상세 데이터: `tests/bench_report_3bp.json`

### 한계

**SAGA가 잘 하는 것:**
- 구조화된 상태(위치, HP, 인벤토리, NPC 관계) 추적
- 프론트엔드 수정 없이 투명한 적용
- 유저 체감 지연 최소화 (1회 호출)
- **.md 캐시 기반의 확장성**: 새로운 .md 파일을 추가하는 것만으로 추적 영역을 넓힐 수 있음 (예: `npc.md`, `world.md`, `faction.md` 등)

**SAGA의 한계:**

1. **추출 정확도**: 모든 것이 State Block 파싱 품질에 달려 있습니다. LLM이 State Block을 잘못 출력하면 상태가 오염됩니다. 경량 LLM 폴백으로 완화하지만 완벽하지 않습니다.

2. **1턴 지연**: 상태 갱신이 비동기이므로, 갱신이 완료되기 전에 다음 턴이 시작되면 이전 상태를 참조할 수 있습니다. (asyncio.Event 락으로 순서 보장하지만, 극단적으로 빠른 입력 시 경합 가능)

3. **에이전트 판단의 한계**: Sub-A/B는 코드 로직이므로, 에이전트가 "이 상황에서는 다르게 판단해야 해"라고 유연하게 대응하기 어렵습니다. 코드 vs 에이전트의 트레이드오프인데, .md 캐시 구조를 잘 설계하면 코드만으로도 커버 범위가 꽤 넓어집니다.

---

## 9. 코히바블랙님 및 선행 연구자분들께

코히바블랙님의 연구일지를 비롯한 선행 연구들에서 영감을 받아, 같은 문제를 Letta를 핵심에 두고 프록시 아키텍처로 풀어본 기록입니다. SAGA의 Curator는 Letta의 Memory Block 패턴을 직접 사용하고 있으며, 매 턴 처리를 코드 로직으로 분리하여 응답 속도를 확보하는 구조입니다.

---

## 10. 만들면서 느낀 것

1. **"어디에 에이전트를 두는가"가 아키텍처의 핵심**입니다. 전부 에이전트에게 맡기면 강력하지만 느리고, 전부 코드로 하면 빠르지만 경직됩니다. 에이전트의 판단력이 진짜 필요한 곳을 찾는 게 핵심이었습니다.

2. **SQLite + 벡터, 둘 다 필요했습니다.** 상태와 관계는 SQLite, 유사도 검색은 벡터 DB — 각각이 다른 질문에 답합니다. 그래프 DB(KuzuDB)는 초기에 사용했으나, 관계 데이터의 규모가 크지 않아 SQLite 테이블로 충분히 대체 가능했습니다.

3. **State Block 의존은 양날의 검**입니다. 구조화된 추출이 가능하지만, LLM이 출력을 실수하면 모든 것이 무너집니다. 경량 LLM 폴백이 있지만, 근본적으로 "LLM의 출력 형식 준수"에 의존하는 건 불안 요소입니다.

4. **비동기 처리가 핵심**이었습니다. 유저 대기 경로에서 무거운 작업을 빼는 것만으로 체감 성능이 달라집니다.

5. **직접 만들지 말고 잘 엮어라.** Letta, ChromaDB, SQLite — 각각 이미 잘 만들어진 도구입니다. SAGA의 가치는 새로운 DB나 메모리 시스템을 발명하는 게 아니라, 기존 도구들을 RP에 맞게 오케스트레이션하는 데 있습니다. Claude Code가 Bash와 grep을 조율하듯, SAGA는 SQLite와 벡터 DB와 Stateful Agent를 조율합니다.

의견이나 제안이 있으시면 댓글로 남겨주세요. 같이 개발해 보고 싶으신 분도 편하게 연락 부탁드립니다.

---

## 11. 참고 자료

**[1]** 코히바블랙 (2025). *Letta를 이용한 장기기억 향상 및 AI 채팅 경험 향상 연구 초록.* [arca.live AI 채팅 채널](https://arca.live/b/characterai/162255622)
**[2]** [Agent Memory Paper List](https://github.com/Shichun-Liu/Agent-Memory-Paper-List) — MemGPT, Graph RAG, RAG 등 이 글에서 언급한 논문들의 종합 목록.
**[3]** Letta (2025). *MemFS: Memory as a File System.* https://docs.letta.com/letta-code/memory

---

*이 글은 [Claude Code](https://claude.ai/claude-code)와 함께 작성되었습니다.*

